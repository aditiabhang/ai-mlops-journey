# Sprint 5 Â· Day 2

## ðŸ”§ The ML Pipeline + AWS AI Services

`90 min` Â· `Concept + AWS service mapping` Â· `AIF-C01 Domain 1 continued`

---

&nbsp;

## Today's Big Picture

> Yesterday you learned WHAT AI is. Today you learn HOW it gets built.
> The ML pipeline is the assembly line â€” data in, trained model out.
> Then we map it to AWS services so you know which tool to use for what.

By the end of today, you'll have:

- âœ… Understand the full ML pipeline (data â†’ train â†’ evaluate â†’ deploy â†’ monitor)
- âœ… Know key model performance metrics (accuracy, F1, AUC)
- âœ… Understand MLOps basics
- âœ… Can name 10+ AWS AI services and when to use each

&nbsp;

---

&nbsp;

## Part 1 â€” The ML Pipeline `25 min`

&nbsp;

### The journey of a model â€” from raw data to production

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. DATA â”‚    â”‚  2. DATA â”‚    â”‚ 3. MODEL â”‚    â”‚  4. EVAL â”‚    â”‚ 5. DEPLOYâ”‚    â”‚ 6. MONI- â”‚
â”‚  COLLECT â”‚â”€â”€â”€â–¶â”‚  PREPARE â”‚â”€â”€â”€â–¶â”‚  TRAIN   â”‚â”€â”€â”€â–¶â”‚  UATE    â”‚â”€â”€â”€â–¶â”‚          â”‚â”€â”€â”€â–¶â”‚  TOR     â”‚
â”‚          â”‚    â”‚          â”‚    â”‚          â”‚    â”‚          â”‚    â”‚          â”‚    â”‚          â”‚
â”‚ â€¢ Gather â”‚    â”‚ â€¢ Clean  â”‚    â”‚ â€¢ Pick   â”‚    â”‚ â€¢ Test   â”‚    â”‚ â€¢ API    â”‚    â”‚ â€¢ Drift  â”‚
â”‚   data   â”‚    â”‚ â€¢ Label  â”‚    â”‚   algo   â”‚    â”‚ â€¢ Metricsâ”‚    â”‚ â€¢ Serve  â”‚    â”‚ â€¢ Retrainâ”‚
â”‚ â€¢ Store  â”‚    â”‚ â€¢ Split  â”‚    â”‚ â€¢ Train  â”‚    â”‚ â€¢ Tune   â”‚    â”‚ â€¢ Scale  â”‚    â”‚ â€¢ Alerts â”‚
â”‚          â”‚    â”‚ â€¢ Featureâ”‚    â”‚ â€¢ Iterateâ”‚    â”‚          â”‚    â”‚          â”‚    â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                                                              â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ feedback loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

&nbsp;

### Each stage explained

| Stage | What happens | Key terms |
|-------|-------------|-----------|
| **1. Data Collection** | Gather raw data from databases, APIs, files, sensors | Data sources, data lake |
| **2. Data Preparation** | Clean, label, transform, split into train/test sets | Feature engineering, data wrangling, train/test split |
| **3. Model Training** | Feed data into an algorithm, adjust weights | Epochs, loss function, hyperparameters |
| **4. Evaluation** | Test the model on data it hasn't seen | Accuracy, precision, recall, F1, AUC |
| **5. Deployment** | Make the model available for use | API endpoint, managed service, container |
| **6. Monitoring** | Watch for performance degradation over time | Model drift, data drift, retraining |

&nbsp;

### The train/test split â€” why it matters

```
Your dataset (1000 emails):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Training set (80%)         â”‚  Test set (20%)      â”‚
â”‚  800 emails                 â”‚  200 emails          â”‚
â”‚                             â”‚                      â”‚
â”‚  Model learns from these    â”‚  Model is tested     â”‚
â”‚                             â”‚  on these (never     â”‚
â”‚                             â”‚  seen during         â”‚
â”‚                             â”‚  training)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Why? If you test on training data, the model just memorizes answers.
That's called OVERFITTING. It looks great in testing, fails in real life.
```

&nbsp;

---

&nbsp;

## Part 2 â€” Model Metrics `15 min`

&nbsp;

### The metrics the exam tests

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MODEL PERFORMANCE METRICS                           â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  ACCURACY  â”‚  â”‚  PRECISION â”‚  â”‚  RECALL    â”‚     â”‚
â”‚  â”‚            â”‚  â”‚            â”‚  â”‚            â”‚     â”‚
â”‚  â”‚  "How      â”‚  â”‚  "When it  â”‚  â”‚  "Did it   â”‚     â”‚
â”‚  â”‚   often    â”‚  â”‚   says yes â”‚  â”‚   find ALL â”‚     â”‚
â”‚  â”‚   right?"  â”‚  â”‚   is it    â”‚  â”‚   the yes  â”‚     â”‚
â”‚  â”‚            â”‚  â”‚   right?"  â”‚  â”‚   cases?"  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚  F1 SCORE  â”‚  â”‚  AUC       â”‚                      â”‚
â”‚  â”‚            â”‚  â”‚            â”‚                      â”‚
â”‚  â”‚  Balance   â”‚  â”‚  Overall   â”‚                      â”‚
â”‚  â”‚  of prec.  â”‚  â”‚  model     â”‚                      â”‚
â”‚  â”‚  & recall  â”‚  â”‚  quality   â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Metric | What it measures | When it matters |
|--------|-----------------|-----------------|
| **Accuracy** | % of correct predictions overall | Balanced datasets |
| **Precision** | Of all "yes" predictions, how many were actually yes? | When false positives are costly (spam filter) |
| **Recall** | Of all actual yes cases, how many did we find? | When missing a case is dangerous (cancer detection) |
| **F1 Score** | Harmonic mean of precision and recall | When you need both |
| **AUC** | Area Under the ROC Curve â€” overall quality | General model comparison |

&nbsp;

### Overfitting vs Underfitting

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                      â”‚
â”‚  UNDERFITTING         GOOD FIT         OVERFITTING   â”‚
â”‚                                                      â”‚
â”‚  Too simple           Just right       Too complex   â”‚
â”‚  Misses patterns      Captures signal  Memorizes     â”‚
â”‚  Bad on training      Good on both     noise         â”‚
â”‚  AND test data        datasets         Great on      â”‚
â”‚                                        training,     â”‚
â”‚  "Can't even          "Nailed it"      FAILS on new  â”‚
â”‚   learn the                            data          â”‚
â”‚   basics"                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

&nbsp;

---

&nbsp;

## Part 3 â€” MLOps `10 min`

&nbsp;

### What is MLOps? (30-second version)

DevOps for machine learning. Same idea â€” automate everything, make it repeatable, monitor it in production. The ML pipeline from Part 1 run as an automated, repeatable system.

| MLOps Concept | One-liner |
|--------------|-----------|
| **Experiment tracking** | Log every model training run â€” parameters, metrics, results |
| **Model registry** | Versioned storage for trained models |
| **Automated pipelines** | Data prep â†’ train â†’ eval â†’ deploy â€” all automated |
| **Model monitoring** | Watch for drift â€” retrain when performance drops |
| **Reproducibility** | Same data + same code = same model every time |
| **Feature store** | Central place for reusable data features |
| **Model drift** | Model gets worse over time as real-world data changes |

&nbsp;

---

&nbsp;

## Part 4 â€” AWS AI/ML Services Map `30 min`

&nbsp;

### The 3 layers of AWS AI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 3: AI SERVICES (no ML knowledge needed)      â”‚
â”‚  Pre-built AI for common tasks                      â”‚
â”‚                                                     â”‚
â”‚  Rekognition Â· Comprehend Â· Transcribe Â· Translate  â”‚
â”‚  Polly Â· Lex Â· Textract Â· Personalize Â· Kendra      â”‚
â”‚  Fraud Detector                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  LAYER 2: ML PLATFORMS (some ML knowledge)          â”‚
â”‚  Build, train, deploy custom models                 â”‚
â”‚                                                     â”‚
â”‚  SageMaker Â· SageMaker JumpStart Â· Bedrock          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  LAYER 1: ML INFRASTRUCTURE (deep ML knowledge)    â”‚
â”‚  Raw compute for training                           â”‚
â”‚                                                     â”‚
â”‚  EC2 (GPU instances) Â· S3 Â· EKS                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **For the exam:** You'll mostly be asked about Layer 3 (which service for which task) and Layer 2 (SageMaker and Bedrock).

&nbsp;

### AWS AI Services â€” the cheat table

| Service | What it does | Use case |
|---------|-------------|----------|
| **Amazon Rekognition** | Image and video analysis | Face detection, object labeling, content moderation |
| **Amazon Comprehend** | NLP â€” text analysis | Sentiment analysis, entity extraction, topic modeling |
| **Amazon Transcribe** | Speech to text | Meeting transcription, call center recordings |
| **Amazon Translate** | Language translation | Translate content to 75+ languages |
| **Amazon Polly** | Text to speech | Voice assistants, audiobook generation |
| **Amazon Lex** | Chatbot builder | Build conversational bots (powers Alexa) |
| **Amazon Textract** | Document data extraction | Pull text/tables from scanned PDFs |
| **Amazon Personalize** | Recommendations | "Customers who bought X also bought Y" |
| **Amazon Kendra** | Intelligent search | Enterprise document search with NLP |
| **Amazon Fraud Detector** | Fraud detection | Online payment fraud, fake account detection |
| **Amazon A2I** | Human review of ML predictions | Human-in-the-loop for low-confidence results |
| **Amazon SageMaker** | Full ML platform | Build, train, deploy custom models |
| **Amazon Bedrock** | Foundation model API | Access Claude, Llama, Titan without managing infra |

&nbsp;

### Quick decision flow for the exam

```
"I need to..."

  Analyze images        â†’ Rekognition
  Understand text       â†’ Comprehend
  Convert speechâ†’text   â†’ Transcribe
  Convert textâ†’speech   â†’ Polly
  Translate languages   â†’ Translate
  Build a chatbot       â†’ Lex
  Extract from docs     â†’ Textract
  Recommend products    â†’ Personalize
  Search documents      â†’ Kendra
  Detect fraud          â†’ Fraud Detector
  Build custom model    â†’ SageMaker
  Use foundation model  â†’ Bedrock
  Need human review     â†’ A2I
```

&nbsp;

---

&nbsp;

## Part 5 â€” Commit `5 min`

```bash
git add sprint-05-ai-foundations/
git commit -m "sprint 5 day 2: ML pipeline, model metrics, MLOps, AWS AI services map"
git push
```

&nbsp;

---

&nbsp;

## âœ… Day 2 Checklist

| | Task |
|---|------|
| â˜ | Can draw the 6-stage ML pipeline from memory |
| â˜ | Know why train/test split matters (overfitting) |
| â˜ | Understand accuracy, precision, recall, F1, AUC |
| â˜ | Know what MLOps is and why model drift matters |
| â˜ | Can name 10+ AWS AI services and match them to use cases |
| â˜ | Pushed to GitHub ðŸŸ© |

&nbsp;

---

&nbsp;

## ðŸ§  Concepts You Now Own

| Concept | One-liner |
|---------|-----------|
| **ML Pipeline** | Data â†’ prepare â†’ train â†’ evaluate â†’ deploy â†’ monitor |
| **Feature engineering** | Transforming raw data into useful model inputs |
| **Train/test split** | 80/20 â€” never test on training data |
| **Overfitting** | Model memorizes training data, fails on new data |
| **Underfitting** | Model too simple, can't learn the pattern |
| **Accuracy** | % of total predictions that are correct |
| **Precision** | Of the "yes" predictions, how many were right? |
| **Recall** | Of the actual "yes" cases, how many did we find? |
| **F1** | Balance of precision and recall |
| **MLOps** | DevOps for ML â€” automate and monitor the pipeline |
| **Model drift** | Model performance degrades as real data changes |
| **SageMaker** | AWS platform to build, train, deploy ML models |
| **Bedrock** | Access foundation models via API â€” no infra to manage |

&nbsp;

---

&nbsp;

> *Next: Generative AI â€” transformers, tokens, and how LLMs actually work under the hood. ðŸ¤–*
