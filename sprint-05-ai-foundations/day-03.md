# Sprint 5 Â· Day 3

## ğŸ¤– Generative AI â€” How LLMs Actually Work

`90 min` Â· `AIF-C01 Domain 2 starts here` Â· `The "how does ChatGPT work?" day`

---

&nbsp;

## Today's Big Picture

> You've used Ollama. You've chatted with LLMs. But how do they work?
> Today you learn transformers, tokens, embeddings, and context windows â€”
> the building blocks of every AI product in 2025.

By the end of today, you'll have:

- âœ… Understand what a foundation model is
- âœ… Know how transformers and attention work (concept level)
- âœ… Understand tokenization and context windows
- âœ… Know what embeddings are and why they matter
- âœ… Understand the foundation model lifecycle (pre-train â†’ fine-tune â†’ deploy)

&nbsp;

---

&nbsp;

## Part 1 â€” Foundation Models `15 min`

&nbsp;

### What is a foundation model? (30-second version)

A foundation model is a massive model trained on a huge amount of general data. You don't build it from scratch â€” you take it and adapt it for your task. Think of it as a college graduate: broadly educated, needs job-specific training.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FOUNDATION MODEL LIFECYCLE                         â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ PRE-     â”‚    â”‚ FINE-    â”‚    â”‚ DEPLOY   â”‚      â”‚
â”‚  â”‚ TRAIN    â”‚â”€â”€â”€â–¶â”‚ TUNE     â”‚â”€â”€â”€â–¶â”‚          â”‚      â”‚
â”‚  â”‚          â”‚    â”‚          â”‚    â”‚          â”‚      â”‚
â”‚  â”‚ Train on â”‚    â”‚ Adapt to â”‚    â”‚ Serve    â”‚      â”‚
â”‚  â”‚ billions â”‚    â”‚ YOUR     â”‚    â”‚ via API  â”‚      â”‚
â”‚  â”‚ of text  â”‚    â”‚ specific â”‚    â”‚          â”‚      â”‚
â”‚  â”‚ pages    â”‚    â”‚ task     â”‚    â”‚          â”‚      â”‚
â”‚  â”‚          â”‚    â”‚          â”‚    â”‚          â”‚      â”‚
â”‚  â”‚ Cost:$$$ â”‚    â”‚ Cost: $  â”‚    â”‚ Cost: $  â”‚      â”‚
â”‚  â”‚ Time:wks â”‚    â”‚ Time:hrs â”‚    â”‚ Time:ms  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                     â”‚
â”‚  Done by:        Done by:        Done by:           â”‚
â”‚  OpenAI, Meta,   YOU (or your    YOU                â”‚
â”‚  Amazon, Google  team)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

&nbsp;

### Key foundation models you should know

| Model | Creator | Known for |
|-------|---------|-----------|
| **GPT-4** | OpenAI | ChatGPT â€” general purpose |
| **Claude** | Anthropic | Safety-focused, long context |
| **Llama** | Meta | Open-source, runs locally |
| **Titan** | Amazon | Built for Bedrock |
| **Gemini** | Google | Multimodal (text + images) |
| **Mistral** | Mistral AI | Open-source, efficient |
| **Stable Diffusion** | Stability AI | Image generation |

&nbsp;

---

&nbsp;

## Part 2 â€” Transformers & Attention `20 min`

&nbsp;

### What is a transformer? (30-second version)

The transformer is the architecture behind every modern LLM. Before transformers, AI processed text word-by-word (slow). Transformers process ALL words at once and figure out which words matter most to each other. That "figuring out" is called **attention**.

```
Old approach (RNN):                 Transformer:
Process one word at a time          Process ALL words at once

"The" â†’ "cat" â†’ "sat" â†’ "on"       "The cat sat on the mat"
  â†“       â†“       â†“       â†“              â†“
Slow. Forgets early words.          Fast. Sees everything at once.
```

&nbsp;

### Attention â€” the key insight

When the model reads "The cat sat on the mat," it needs to know that "sat" is connected to "cat" (who sat?) and "mat" (sat where?). Attention scores tell the model which words to focus on.

```
Input: "The bank was on the river bank"

  "bank" (first) â†’ attention looks at "river" â†’ aha, it's a RIVER bank
  "bank" (second) â†’ attention looks at context â†’ same meaning confirmed

Without attention, the model can't tell the two "bank"s apart.
```

&nbsp;

### The transformer architecture (simplified)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: "What is Kubernetes?"       â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  1. TOKENIZE                â”‚    â”‚
â”‚  â”‚  Split into tokens          â”‚    â”‚
â”‚  â”‚  ["What", "is", "Kub",     â”‚    â”‚
â”‚  â”‚   "ern", "etes", "?"]      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                 â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  2. EMBED                   â”‚    â”‚
â”‚  â”‚  Convert tokens to numbers  â”‚    â”‚
â”‚  â”‚  (vectors in high-dim space)â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                 â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  3. ATTENTION LAYERS        â”‚    â”‚
â”‚  â”‚  "Which tokens relate to    â”‚    â”‚
â”‚  â”‚   which?" (many layers)     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                 â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  4. PREDICT NEXT TOKEN      â”‚    â”‚
â”‚  â”‚  "Kubernetes" â†’ "is" â†’ "an" â”‚    â”‚
â”‚  â”‚  â†’ "open" â†’ "source" â†’ ... â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                     â”‚
â”‚  OUTPUT: "Kubernetes is an open     â”‚
â”‚  source container orchestration..." â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> You don't need to know the math. Know the FLOW: tokenize â†’ embed â†’ attention â†’ predict.

&nbsp;

---

&nbsp;

## Part 3 â€” Tokens, Context Windows, Embeddings `20 min`

&nbsp;

### Tokens

Tokens are how models read text. A token is NOT a word â€” it's a piece of a word. Common words = 1 token. Long/rare words = multiple tokens.

```
"Hello"           â†’ 1 token
"Kubernetes"      â†’ 3 tokens ("Kub", "ern", "etes")
"I love Python"   â†’ 3 tokens
"CrashLoopBackOff" â†’ 5 tokens

Rule of thumb: 1 token â‰ˆ 4 characters â‰ˆ 0.75 words
```

> **Why tokens matter for the exam:** Pricing is per token. Context windows are measured in tokens. A model with a 128K context window can "see" ~96,000 words at once.

&nbsp;

### Context Window

The maximum number of tokens a model can process in one conversation. Everything â€” your question, the model's previous answers, system prompts â€” all counts toward the limit.

| Model | Context Window |
|-------|---------------|
| GPT-4 | 128K tokens |
| Claude 3 | 200K tokens |
| Llama 3.2 | 128K tokens |
| Titan | 8K-32K tokens |

```
Context window is like the model's short-term memory:

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  System prompt: 200 tokens           â”‚
  â”‚  Your message 1: 100 tokens          â”‚
  â”‚  Model response 1: 300 tokens        â”‚
  â”‚  Your message 2: 50 tokens           â”‚
  â”‚  Model response 2: ???               â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚
  â”‚  Total so far: 650 tokens            â”‚
  â”‚  Remaining: 127,350 tokens           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

&nbsp;

### Embeddings

An embedding is a way to represent text as numbers (a vector). Similar meanings end up close together in this number space.

```
"king"  â†’ [0.9, 0.2, 0.8, ...]
"queen" â†’ [0.9, 0.2, 0.7, ...]  â† very close to "king"!
"cat"   â†’ [0.1, 0.7, 0.3, ...]  â† far away from "king"
```

> **Why embeddings matter:** They power search, recommendations, and RAG (Retrieval Augmented Generation). You convert your documents into embeddings, store them in a vector database, and search by meaning instead of keywords.

&nbsp;

---

&nbsp;

## Part 4 â€” Prompt Engineering Deep Dive `15 min`

&nbsp;

### You already know the basics from Sprint 3. Here's the exam-level detail:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PROMPT ENGINEERING TECHNIQUES                       â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ ZERO-SHOT  â”‚  â”‚ FEW-SHOT   â”‚  â”‚ CHAIN-OF-  â”‚     â”‚
â”‚  â”‚            â”‚  â”‚            â”‚  â”‚ THOUGHT    â”‚     â”‚
â”‚  â”‚ Just ask.  â”‚  â”‚ Give 2-3   â”‚  â”‚ "Think     â”‚     â”‚
â”‚  â”‚ No examplesâ”‚  â”‚ examples   â”‚  â”‚  step by   â”‚     â”‚
â”‚  â”‚            â”‚  â”‚ first.     â”‚  â”‚  step"     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚ SYSTEM     â”‚  â”‚ RAG        â”‚                      â”‚
â”‚  â”‚ PROMPT     â”‚  â”‚            â”‚                      â”‚
â”‚  â”‚            â”‚  â”‚ Retrieve   â”‚                      â”‚
â”‚  â”‚ Hidden     â”‚  â”‚ relevant   â”‚                      â”‚
â”‚  â”‚ persona /  â”‚  â”‚ docs THEN  â”‚                      â”‚
â”‚  â”‚ rules      â”‚  â”‚ ask model  â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

&nbsp;

### New for the exam: inference parameters

| Parameter | What it does | Low value | High value |
|-----------|-------------|-----------|------------|
| **Temperature** | Controls randomness | More focused, deterministic | More creative, varied |
| **Top-p** | Limits token selection pool | Fewer options, safer | More options, diverse |
| **Max tokens** | Output length limit | Short response | Long response |
| **Stop sequences** | When to stop generating | â€” | â€” |

> **Exam tip:** Temperature is the most tested parameter. Low temp (0.1) = factual, consistent. High temp (0.9) = creative, varied. For customer support = low. For creative writing = high.

&nbsp;

---

&nbsp;

## Part 5 â€” GenAI Use Cases `5 min`

&nbsp;

### What generative AI can do (exam loves these)

| Use Case | Example |
|----------|---------|
| **Text generation** | Write emails, articles, code |
| **Summarization** | Condense long documents |
| **Translation** | Convert between languages |
| **Code generation** | Write Python, SQL, YAML |
| **Chatbots** | Customer service, support |
| **Image generation** | Create visuals from text prompts |
| **Search** | Semantic search â€” find by meaning, not keywords |
| **Data extraction** | Pull structured data from unstructured text |

&nbsp;

### What it CAN'T do well (exam tests these too)

| Limitation | What it means |
|-----------|--------------|
| **Hallucinations** | Makes up facts that sound convincing |
| **No real-time data** | Only knows what it was trained on |
| **Bias** | Reflects biases in training data |
| **Not deterministic** | Same prompt can give different answers |
| **Context limits** | Forgets beyond the context window |

&nbsp;

---

&nbsp;

## Part 6 â€” Commit `5 min`

```bash
git add sprint-05-ai-foundations/
git commit -m "sprint 5 day 3: generative AI â€” transformers, tokens, embeddings, prompt eng"
git push
```

&nbsp;

---

&nbsp;

## âœ… Day 3 Checklist

| | Task |
|---|------|
| â˜ | Know what a foundation model is and name 5+ |
| â˜ | Can explain transformers and attention at a concept level |
| â˜ | Understand tokens, context windows, and embeddings |
| â˜ | Know all prompt engineering techniques for the exam |
| â˜ | Understand temperature and inference parameters |
| â˜ | Can name GenAI use cases AND limitations |
| â˜ | Pushed to GitHub ğŸŸ© |

&nbsp;

---

&nbsp;

## ğŸ§  Concepts You Now Own

| Concept | One-liner |
|---------|-----------|
| **Foundation model** | Massive model pre-trained on general data â€” you adapt it |
| **Transformer** | Architecture that processes all words at once using attention |
| **Attention** | "Which words matter most to each other?" |
| **Token** | A piece of a word â€” how models read text (~4 chars) |
| **Context window** | Max tokens a model can see in one conversation |
| **Embedding** | Text as numbers â€” similar meanings = similar numbers |
| **Vector database** | Stores embeddings for similarity search |
| **Temperature** | Controls randomness: low = focused, high = creative |
| **Hallucination** | Model confidently makes up wrong information |
| **RAG** | Retrieve documents first, then ask the model |

&nbsp;

---

&nbsp;

> *Next: AWS GenAI services â€” Bedrock, SageMaker JumpStart, Amazon Q, and how they all fit together. â˜ï¸*
