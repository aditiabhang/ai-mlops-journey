# Sprint 6 Â· Day 2

## ðŸ”¬ Domain 3 Continued â€” Fine-tuning, Evaluation & Model Serving

`90 min` Â· `AIF-C01 Domain 3 Part 2` Â· `The "how models get better" day`

---

&nbsp;

## Today's Big Picture

> Yesterday was about USING models. Today is about IMPROVING and EVALUATING them.
> How do you make a model better at your specific task? How do you measure if it's working?
> And how do you serve it in production?

By the end of today, you'll have:

- âœ… Understand the fine-tuning process and methods
- âœ… Know how to prepare data for fine-tuning
- âœ… Understand foundation model evaluation metrics (ROUGE, BLEU, BERTScore)
- âœ… Know how to serve a model in production
- âœ… Built a simple model serving API (FastAPI)

&nbsp;

---

&nbsp;

## Part 1 â€” Fine-tuning Foundation Models `25 min`

&nbsp;

### What is fine-tuning? (30-second version)

Pre-training teaches a model everything. Fine-tuning teaches it YOUR thing. It's the difference between a college graduate (general knowledge) and an employee after onboarding (knows your company's way of doing things).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CUSTOMIZATION SPECTRUM                              â”‚
â”‚                                                      â”‚
â”‚  No change                                 Full      â”‚
â”‚  â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  rebuild   â”‚
â”‚                                                      â”‚
â”‚  Prompt         In-context    Fine-        Contin-   â”‚
â”‚  Engineering    Learning      tuning       uous      â”‚
â”‚                                            Pre-      â”‚
â”‚  "Ask better"   "Give         "Retrain     training  â”‚
â”‚                  examples      some                   â”‚
â”‚  Cost: Free     in prompt"    weights"    "Train     â”‚
â”‚  Time: Instant                             from      â”‚
â”‚                 Cost: $       Cost: $$     more data" â”‚
â”‚                 Time: Instant Time: Hours             â”‚
â”‚                                           Cost: $$$$ â”‚
â”‚                                           Time: Days â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

&nbsp;

### Fine-tuning methods the exam tests

| Method | What it does | When to use |
|--------|-------------|-------------|
| **Instruction tuning** | Train the model to follow specific instructions | Customer service bots, task-specific assistants |
| **Domain adaptation** | Train on domain-specific data (medical, legal) | When the model doesn't know your industry's language |
| **Transfer learning** | Take knowledge from one task, apply to another | Limited data for your specific task |
| **Continuous pre-training** | Keep training the base model on new data | Model needs updated knowledge |
| **RLHF** | Reinforcement Learning from Human Feedback â€” humans rate outputs | Making responses more helpful and safe |

&nbsp;

### Preparing data for fine-tuning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATA PREPARATION CHECKLIST                          â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ CURATE     â”‚  â”‚ QUALITY    â”‚  â”‚ FORMAT     â”‚     â”‚
â”‚  â”‚            â”‚  â”‚            â”‚  â”‚            â”‚     â”‚
â”‚  â”‚ â€¢ Relevant â”‚  â”‚ â€¢ Clean    â”‚  â”‚ â€¢ Prompt + â”‚     â”‚
â”‚  â”‚ â€¢ Diverse  â”‚  â”‚ â€¢ Accurate â”‚  â”‚   completionâ”‚    â”‚
â”‚  â”‚ â€¢ Represen-â”‚  â”‚ â€¢ No PII   â”‚  â”‚   pairs     â”‚    â”‚
â”‚  â”‚   tative   â”‚  â”‚ â€¢ No bias  â”‚  â”‚ â€¢ JSONL    â”‚     â”‚
â”‚  â”‚ â€¢ Enough   â”‚  â”‚ â€¢ Labeled  â”‚  â”‚ â€¢ Consistentâ”‚    â”‚
â”‚  â”‚   volume   â”‚  â”‚            â”‚  â”‚            â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Consideration | What to know |
|--------------|-------------|
| **Data curation** | Select relevant, high-quality examples |
| **Data governance** | Ensure compliance, track data lineage |
| **Size** | More data = better fine-tuning (typically 100s-1000s of examples) |
| **Labeling** | Human-reviewed labels for quality |
| **Representativeness** | Data should reflect real-world distribution |
| **RLHF** | Human raters score model outputs to improve alignment |

&nbsp;

---

&nbsp;

## Part 2 â€” Model Evaluation `20 min`

&nbsp;

### How do you know if a model is good?

Two approaches: automated metrics and human evaluation.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MODEL EVALUATION                                    â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  AUTOMATED METRICS   â”‚  â”‚  HUMAN EVALUATION    â”‚  â”‚
â”‚  â”‚                      â”‚  â”‚                      â”‚  â”‚
â”‚  â”‚  Fast, cheap,        â”‚  â”‚  Slow, expensive,    â”‚  â”‚
â”‚  â”‚  consistent          â”‚  â”‚  but captures nuance â”‚  â”‚
â”‚  â”‚                      â”‚  â”‚                      â”‚  â”‚
â”‚  â”‚  â€¢ ROUGE             â”‚  â”‚  â€¢ Relevance rating  â”‚  â”‚
â”‚  â”‚  â€¢ BLEU              â”‚  â”‚  â€¢ Helpfulness score â”‚  â”‚
â”‚  â”‚  â€¢ BERTScore         â”‚  â”‚  â€¢ Safety review     â”‚  â”‚
â”‚  â”‚  â€¢ Perplexity        â”‚  â”‚  â€¢ A/B testing       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

&nbsp;

### Automated metrics â€” the ones the exam tests

| Metric | Stands for | Measures | Best for |
|--------|-----------|----------|----------|
| **ROUGE** | Recall-Oriented Understudy for Gisting Evaluation | Overlap between model output and reference text | **Summarization** |
| **BLEU** | Bilingual Evaluation Understudy | Precision of generated text vs reference | **Translation** |
| **BERTScore** | BERT-based similarity | Semantic similarity using embeddings | **General text quality** |
| **Perplexity** | â€” | How surprised the model is by text (lower = better) | **Language modeling** |

> **Memory trick:**
> - **ROUGE** = **R**ecall â†’ **S**ummarization (both start with S... close enough)
> - **BLEU** = **B**ilingual â†’ **T**ranslation
> - **BERTScore** = **B**EST for general comparison

&nbsp;

### Business metrics (also tested)

| Metric | What it measures |
|--------|-----------------|
| **Cost per user** | How much each AI interaction costs |
| **Development costs** | Total cost to build and deploy |
| **Customer feedback** | User satisfaction scores |
| **ROI** | Return on investment â€” value vs cost |
| **User engagement** | How much users interact with the AI |
| **Task completion rate** | % of tasks the AI successfully handles |

&nbsp;

---

&nbsp;

## Part 3 â€” Model Serving `15 min`

&nbsp;

### How models get to production

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MODEL SERVING OPTIONS                               â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  MANAGED API       â”‚  â”‚  SELF-HOSTED           â”‚  â”‚
â”‚  â”‚                    â”‚  â”‚                        â”‚  â”‚
â”‚  â”‚  â€¢ Bedrock         â”‚  â”‚  â€¢ FastAPI + Docker    â”‚  â”‚
â”‚  â”‚  â€¢ SageMaker       â”‚  â”‚  â€¢ On EC2 or EKS       â”‚  â”‚
â”‚  â”‚    endpoint        â”‚  â”‚  â€¢ Full control        â”‚  â”‚
â”‚  â”‚  â€¢ No infra mgmt   â”‚  â”‚  â€¢ You manage scaling  â”‚  â”‚
â”‚  â”‚  â€¢ Pay per use     â”‚  â”‚  â€¢ Pay for compute     â”‚  â”‚
â”‚  â”‚                    â”‚  â”‚                        â”‚  â”‚
â”‚  â”‚  Best for:         â”‚  â”‚  Best for:             â”‚  â”‚
â”‚  â”‚  Most use cases    â”‚  â”‚  Custom models,        â”‚  â”‚
â”‚  â”‚                    â”‚  â”‚  strict requirements   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

&nbsp;

---

&nbsp;

## Part 4 â€” Mini-Project: Model Serving API `20 min`

&nbsp;

### Build a simple model serving API with FastAPI

This demonstrates the concept of serving a model via REST API â€” the same pattern Bedrock and SageMaker use, just simpler.

**1 â†’** Create the project

```bash
mkdir -p ~/projects/ai-mlops-journey/projects/02-model-serving
cd ~/projects/ai-mlops-journey/projects/02-model-serving
python3 -m venv .venv
source .venv/bin/activate
pip install fastapi uvicorn requests
```

&nbsp;

**2 â†’** Create `app.py`

```python
from fastapi import FastAPI
from pydantic import BaseModel
import requests

app = FastAPI(title="Model Serving API")


class PromptRequest(BaseModel):
    prompt: str
    model: str = "llama3.2"


class PromptResponse(BaseModel):
    model: str
    prompt: str
    response: str


@app.get("/health")
def health():
    return {"status": "healthy"}


@app.post("/generate", response_model=PromptResponse)
def generate(req: PromptRequest):
    result = requests.post("http://localhost:11434/api/chat", json={
        "model": req.model,
        "messages": [{"role": "user", "content": req.prompt}],
        "stream": False
    })
    answer = result.json()["message"]["content"]
    return PromptResponse(model=req.model, prompt=req.prompt, response=answer)
```

&nbsp;

**3 â†’** Run it (make sure `ollama serve` is running in another tab)

```bash
uvicorn app:app --reload --port 8000
```

&nbsp;

**4 â†’** Test it

```bash
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "What is RAG in 2 sentences?"}'
```

> You just built a model serving API. This is exactly what happens behind Bedrock and SageMaker â€” a REST API that wraps a model.

&nbsp;

**5 â†’** Visit `http://localhost:8000/docs` â€” FastAPI auto-generates interactive API docs. Click "Try it out" to test from your browser.

&nbsp;

---

&nbsp;

## Part 5 â€” Commit `5 min`

```bash
cd ~/projects/ai-mlops-journey
echo ".venv/" > projects/02-model-serving/.gitignore
git add projects/02-model-serving/ sprint-06-ai-exam-prep/
git commit -m "sprint 6 day 2: fine-tuning, eval metrics, model serving API"
git push
```

&nbsp;

---

&nbsp;

## âœ… Day 2 Checklist

| | Task |
|---|------|
| â˜ | Can explain fine-tuning methods (instruction tuning, RLHF, domain adaptation) |
| â˜ | Know how to prepare data for fine-tuning |
| â˜ | Can name ROUGE, BLEU, BERTScore and what each measures |
| â˜ | Understand managed API vs self-hosted model serving |
| â˜ | Built a FastAPI model serving API |
| â˜ | Pushed to GitHub ðŸŸ© |

&nbsp;

---

&nbsp;

## ðŸ§  Concepts You Now Own

| Concept | One-liner |
|---------|-----------|
| **Fine-tuning** | Retrain some model weights on your specific data |
| **Instruction tuning** | Teach model to follow instructions better |
| **RLHF** | Humans rate outputs to improve alignment |
| **Transfer learning** | Apply knowledge from one task to another |
| **ROUGE** | Summarization metric â€” recall-based overlap |
| **BLEU** | Translation metric â€” precision-based overlap |
| **BERTScore** | Semantic similarity using embeddings |
| **Perplexity** | How "surprised" the model is â€” lower is better |
| **Model serving** | Making a model available via API for predictions |
| **FastAPI** | Python framework for building REST APIs â€” fast and modern |

&nbsp;

---

&nbsp;

> *Next: Responsible AI + Security & Compliance â€” the "don't be evil" domains. ðŸ”’*
