# Sprint 3 Â· Day 3

## ğŸ¤– Ollama â€” Run an LLM on Your Laptop

`90 min` Â· `Needs: 8GB+ RAM, ~5GB disk` Â· `The moment AI becomes real`

---

&nbsp;

## Today's Big Picture

> Forget ChatGPT. Forget API keys. Forget the cloud.
> Today you download a large language model and run it on YOUR machine.
> No internet needed. No data sent anywhere. Just you and a neural network.

By the end of today, you'll have:

- âœ… Ollama installed and running
- âœ… Chatted with at least 2 different models
- âœ… Compared model sizes, speeds, and quality
- âœ… Understand what's actually happening when you "chat with AI"
- âœ… Tried basic prompt engineering techniques

&nbsp;

---

&nbsp;

## Part 1 â€” Install Ollama `10 min`

&nbsp;

### What is Ollama? (30-second version)

Ollama makes it dead simple to run LLMs locally. One command to download a model, one command to chat with it. No Python setup, no GPU drivers, no config files.

```
What Ollama does for you:

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Without Ollama:                     â”‚
  â”‚  â€¢ Download model weights (4GB+)     â”‚
  â”‚  â€¢ Install PyTorch                   â”‚
  â”‚  â€¢ Configure CUDA/Metal              â”‚
  â”‚  â€¢ Write inference code              â”‚
  â”‚  â€¢ Handle tokenization               â”‚
  â”‚  â€¢ 2 hours of setup                  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    vs
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  With Ollama:                        â”‚
  â”‚  â€¢ brew install ollama               â”‚
  â”‚  â€¢ ollama run llama3.2               â”‚
  â”‚  â€¢ Done. You're chatting.            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

&nbsp;

### Do This

**1 â†’** Install Ollama

```bash
# Mac
brew install ollama
```

> Or download from [ollama.com](https://ollama.com)

&nbsp;

**2 â†’** Start the Ollama server

```bash
ollama serve
```

> Leave this running in a terminal tab. Open a new tab for the next steps.

&nbsp;

**3 â†’** Pull and run your first model

```bash
ollama run llama3.2
```

> This downloads ~2GB and starts a chat. Type anything. You're talking to an LLM on your own machine. ğŸ¤¯

&nbsp;

**4 â†’** Try asking it something

```
>>> Explain Kubernetes pods to a 5-year-old
>>> Write a Python function that checks if a number is prime
>>> What's the difference between Docker and Podman?
```

> Type `/bye` to exit the chat.

&nbsp;

---

&nbsp;

## Part 2 â€” What's Actually Happening? `10 min`

&nbsp;

### Under the hood (30-second version)

When you type a question, the model isn't "thinking." It's predicting the next word, one at a time, based on patterns it learned from billions of text examples.

```
You type:  "What is a pod in"

Model sees: ["What", "is", "a", "pod", "in"]

Model predicts next word probabilities:
  "Kubernetes"  â†’ 82%
  "Docker"      â†’ 8%
  "biology"     â†’ 5%
  "cooking"     â†’ 2%
  ...

Picks "Kubernetes" â†’ then predicts the NEXT word â†’ repeats until done
```

&nbsp;

### Key terms you just learned by doing

| Term | What it means |
|------|--------------|
| **Model** | The brain â€” a file full of learned patterns (weights) |
| **Parameters** | The numbers inside the model â€” more = smarter but slower |
| **Inference** | Running a model to get an answer (not training it) |
| **Token** | A piece of a word â€” "Kubernetes" = ~3 tokens |
| **Context window** | How much text the model can "remember" in one conversation |

&nbsp;

---

&nbsp;

## Part 3 â€” Compare Models `25 min`

&nbsp;

### Why different models?

Models come in different sizes. Bigger = smarter but slower. Smaller = faster but less capable. Picking the right model for the job is a real-world skill.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Size Spectrum                             â”‚
â”‚                                                  â”‚
â”‚  Small              Medium             Large     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 1-3B â”‚          â”‚ 7-8B â”‚          â”‚ 70B+ â”‚   â”‚
â”‚  â”‚      â”‚          â”‚      â”‚          â”‚      â”‚   â”‚
â”‚  â”‚ Fast â”‚          â”‚ Good â”‚          â”‚ Best â”‚   â”‚
â”‚  â”‚ OK   â”‚          â”‚balanceâ”‚         â”‚ Slow â”‚   â”‚
â”‚  â”‚qualityâ”‚         â”‚      â”‚          â”‚ Needsâ”‚   â”‚
â”‚  â”‚      â”‚          â”‚      â”‚          â”‚ GPU  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  phi3              llama3.2          llama3.1   â”‚
â”‚  gemma:2b          mistral           (skip)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

&nbsp;

### Do This â€” Try 3 models

**1 â†’** You already have llama3.2. Now try Mistral:

```bash
ollama run mistral
```

Ask it the same questions you asked llama3.2. Notice the difference in style and quality.

&nbsp;

**2 â†’** Try a small, fast model:

```bash
ollama run phi3
```

> Phi3 is Microsoft's small model â€” fast but less capable. Great for quick tasks.

&nbsp;

**3 â†’** Compare them â€” fill this in as you test

| | llama3.2 | mistral | phi3 |
|--|----------|---------|------|
| **Size on disk** | ~2GB | ~4GB | ~2GB |
| **Speed** | (your notes) | (your notes) | (your notes) |
| **Answer quality** | (your notes) | (your notes) | (your notes) |
| **Best for** | (your notes) | (your notes) | (your notes) |

> This comparison skill matters for the AWS AI Practitioner exam â€” you'll be asked when to choose different model sizes.

&nbsp;

---

&nbsp;

## Part 4 â€” Prompt Engineering Basics `20 min`

&nbsp;

### What is prompt engineering? (30-second version)

The same model gives wildly different answers depending on HOW you ask. Prompt engineering is the art of asking better questions.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Same model, different prompts:                  â”‚
â”‚                                                  â”‚
â”‚  Bad prompt:                                     â”‚
â”‚  "Tell me about Python"                          â”‚
â”‚   â†’ Generic 500-word essay ğŸ˜´                    â”‚
â”‚                                                  â”‚
â”‚  Good prompt:                                    â”‚
â”‚  "Explain Python virtual environments in         â”‚
â”‚   3 bullet points. Audience: someone who         â”‚
â”‚   knows JavaScript but not Python."              â”‚
â”‚   â†’ Focused, useful answer ğŸ¯                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

&nbsp;

### Techniques to try

Pick any model and try each of these in the chat:

&nbsp;

**1 â†’** Zero-shot (just ask directly â€” no examples)

```
>>> What is a Kubernetes ConfigMap?
```

&nbsp;

**2 â†’** Few-shot (give examples first, then ask)

```
>>> I'll give you K8s terms and you explain them in one sentence:
>>> Pod: The smallest deployable unit in Kubernetes
>>> Service: A stable network address for a set of pods
>>> Now explain: ReplicaSet
```

&nbsp;

**3 â†’** Chain-of-thought (ask it to think step by step)

```
>>> A pod is in CrashLoopBackOff status. Walk me through the debugging steps
>>> one by one, explaining why each step matters.
```

&nbsp;

**4 â†’** Role-based (give it a persona)

```
>>> You are a senior Kubernetes administrator. A junior engineer asks you
>>> why their deployment isn't scaling. What questions do you ask them?
```

&nbsp;

### The pattern

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Zero-shot  â”‚     â”‚  Few-shot   â”‚     â”‚  Chain-of-   â”‚
â”‚             â”‚     â”‚             â”‚     â”‚  thought     â”‚
â”‚  Just ask   â”‚     â”‚  Give       â”‚     â”‚  "Think      â”‚
â”‚             â”‚â”€â”€â”€â”€â–¶â”‚  examples   â”‚â”€â”€â”€â”€â–¶â”‚  step by     â”‚
â”‚  Simplest   â”‚     â”‚  first      â”‚     â”‚  step"       â”‚
â”‚  approach   â”‚     â”‚             â”‚     â”‚              â”‚
â”‚             â”‚     â”‚  Better     â”‚     â”‚  Best for    â”‚
â”‚             â”‚     â”‚  accuracy   â”‚     â”‚  reasoning   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

&nbsp;

---

&nbsp;

## Part 5 â€” Commit `10 min`

Write notes on which model you liked best and what prompt techniques worked.

```bash
git add sprint-03-cloud-native/
git commit -m "sprint 3 day 3: ollama installed, compared 3 LLMs, prompt engineering"
git push
```

&nbsp;

---

&nbsp;

## âœ… Day 3 Checklist

| | Task |
|---|------|
| â˜ | Ollama installed and running |
| â˜ | Chatted with llama3.2 |
| â˜ | Tried at least 2 other models (mistral, phi3) |
| â˜ | Filled in the model comparison table |
| â˜ | Tried zero-shot, few-shot, and chain-of-thought prompting |
| â˜ | Pushed notes to GitHub ğŸŸ© |

&nbsp;

---

&nbsp;

## ğŸ§  Concepts You Now Own

| Concept | One-liner |
|---------|-----------|
| **LLM** | Large Language Model â€” predicts the next word, really well |
| **Ollama** | Tool that runs LLMs locally with one command |
| **Parameters (B)** | Billions of numbers inside the model â€” bigger = smarter but slower |
| **Inference** | Running a model to get answers (vs training it) |
| **Token** | A piece of a word â€” how models read text |
| **Context window** | How much text the model can "see" at once |
| **Prompt engineering** | The art of asking better questions to get better answers |
| **Zero-shot** | Just ask â€” no examples given |
| **Few-shot** | Give examples first, then ask |
| **Chain-of-thought** | "Think step by step" â€” improves reasoning |

&nbsp;

---

&nbsp;

> *Next: You turn this into a Python project â€” a CLI chatbot that saves conversations. ğŸğŸ¤–*
